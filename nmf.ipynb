{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from surprise import Dataset,Reader,NMF,accuracy\n",
    "from surprise.model_selection import train_test_split,KFold\n",
    "from utils import Dataloader\n",
    "\n",
    "import joblib\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"./data/\"\n",
    "users_df = Dataloader.load_users(DIR_PATH)\n",
    "ratings_df = Dataloader.load_ratings(DIR_PATH)\n",
    "movies_df = Dataloader.load_movies(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_model = NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_model.fit(trainset)\n",
    "NMF_model_predictions = NMF_model.test(testset)\n",
    "NMF_model_predictions_df = pd.DataFrame(NMF_model_predictions, columns=['uid', 'iid', 'r_ui', 'est', 'details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=5, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9187\n",
      "MAE:  0.7258\n",
      "RMSE: 0.9187241307434599\n",
      "MAE: 0.7257540006519875\n"
     ]
    }
   ],
   "source": [
    "rmse = accuracy.rmse(NMF_model_predictions)\n",
    "mae = accuracy.mae(NMF_model_predictions)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@5(NMF_model): 0.7787733245004048\n",
      "Average Recall@5(NMF_model): 0.3677391452138514\n"
     ]
    }
   ],
   "source": [
    "NMF_model_predictions_list = NMF_model_predictions_df.values.tolist()\n",
    "\n",
    "# Precision@K와 Recall@K 계산\n",
    "NMF_model_precisions, NMF_model_recalls = precision_recall_at_k(NMF_model_predictions_list, k=5, threshold=3.5)\n",
    "\n",
    "# Precision과 Recall 평균 계산\n",
    "NMF_model_avg_precision = sum(prec for prec in NMF_model_precisions.values()) / len(NMF_model_precisions)\n",
    "NMF_model_avg_recall = sum(rec for rec in NMF_model_recalls.values()) / len(NMF_model_recalls)\n",
    "\n",
    "print(\"Average Precision@5(NMF_model):\", NMF_model_avg_precision)\n",
    "print(\"Average Recall@5(NMF_model):\", NMF_model_avg_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "62",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
